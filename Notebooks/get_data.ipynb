{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from time import sleep\n",
    "# import cssselect\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants for use later\n",
    "BASE_URL = \"https://www.mailleartisans.org/weaves/weavedisplay.php?key=\"\n",
    "FIRST_ARTICLE = 1\n",
    "LAST_ARTICLE = 1487"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define Function To Download Data From M.A.I.L.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create function to download articles\n",
    "def download_articles(articles: iter) -> None:\n",
    "    for article in articles:\n",
    "        for i in range(1, 4):\n",
    "            sleep(10)\n",
    "            response = requests.get(f\"{BASE_URL}{article}\")\n",
    "            now = datetime.datetime.now()\n",
    "            if response.status_code == 200:\n",
    "                print(f'{now}: Article {article} attempt {i}/3 succeeded moving on in 10 seconds.')\n",
    "                with open(f'../articles/article_{article}.html', mode='wb') as outfile:\n",
    "                    outfile.write(response.content)\n",
    "                break\n",
    "            elif i < 3:\n",
    "                print(f'{now}: Article {article} attempt {i}/3 failed trying again in 10 seconds.')\n",
    "            else:\n",
    "                print(f'{now}: Article {article} attempt {i}/3 failed moving on in 10 seconds.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Download All Articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-06 20:34:39.459105: Article 1 attempt 1/3 succeeded moving on in 10 seconds.\n",
      "2023-04-06 20:34:49.623323: Article 2 attempt 1/3 succeeded moving on in 10 seconds.\n",
      "2023-04-06 20:34:59.781630: Article 3 attempt 1/3 succeeded moving on in 10 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "download_articles(range(FIRST_ARTICLE, LAST_ARTICLE+1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function To Get Data From Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_articles_and_tags(tree) -> dict:\n",
    "    out = dict()\n",
    "\n",
    "    # Get Article and Tag information\n",
    "    path = '/html/body/div[5]/div/table/tr/td[2]/div[2]//text()'\n",
    "    lines = [i for i in tree.xpath(path) if i not in ('[', ']')]\n",
    "\n",
    "    ## Get indicies to split the tag and article data\n",
    "    tags_start = lines.index('Weave Tags (Click to Search Weaves) ')\n",
    "    article_start = lines.index('Related Articles ')\n",
    "    gallery_start = lines.index('Random Gallery Items Tagged as using this Weave ')\n",
    "\n",
    "\n",
    "    # Get Articles\n",
    "\n",
    "    ## Get only article lines\n",
    "    article_lines = lines[article_start+1:gallery_start]\n",
    "\n",
    "    ## Get Article headings\n",
    "    path = '/html/body/div[5]/div/table/tr/td[2]/div[2]/b//text()'\n",
    "    headings = tree.xpath(path)\n",
    "\n",
    "    if len(headings) > 0:\n",
    "        ## Parse list and headings into dict of lists of articles by heading\n",
    "\n",
    "        ### Set starting values\n",
    "        tmp = list()\n",
    "        level = list()\n",
    "        start = True\n",
    "\n",
    "        ### Split lines into on list per heading\n",
    "        for i in article_lines:\n",
    "            if (i in headings) and start:   \n",
    "                level = list()\n",
    "                level.append(i)\n",
    "                start = False\n",
    "            elif (i in headings) and not start:\n",
    "                tmp.append(level)\n",
    "                level = list()\n",
    "                level.append(i)\n",
    "            else:\n",
    "                level.append(i)\n",
    "        tmp.append(level)\n",
    "\n",
    "        ## Clean up list into list of article titles\n",
    "        lol = [''.join(i).split('\\n') for i in tmp]\n",
    "\n",
    "        ### Turn list into dictionary\n",
    "        tmp = dict()\n",
    "        for i in lol:\n",
    "            tmp[i[0]] = [j for j in i[1:] if j != '']\n",
    "\n",
    "        out['Articles'] = tmp\n",
    "    else:\n",
    "        out['Articles'] = None\n",
    "\n",
    "    # Get Tags\n",
    "    \n",
    "    ## Get tag lines\n",
    "    tag_lines = lines[tags_start+1:article_start]\n",
    "    \n",
    "    ## Remove unecessary info and clean tag lines\n",
    "    out['Tags'] = [i.strip() for i in tag_lines if i not in ('\\n', ', ')] \n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def article_parser(article: bytes) -> dict:\n",
    "    # Create output dict\n",
    "    out = dict()\n",
    "\n",
    "    # Convert article into an etree\n",
    "    soup = BeautifulSoup(article, 'html.parser')\n",
    "    dom = etree.HTML(str(soup.html))\n",
    "\n",
    "\n",
    "    # Parse etree to find values of interest\n",
    "\n",
    "    ## Determine if an article exists\n",
    "    if len(dom.xpath('/html/body/div[5]/div/h2')) > 0:\n",
    "        out = {\n",
    "            'Weave Title':   None,\n",
    "            'Max AR':        None,\n",
    "            'Ideal AR':      None,\n",
    "            'Min AR':        None,\n",
    "            'Date Uploaded': None,\n",
    "            'Last Edited':   None,\n",
    "            'Articles':      None,\n",
    "            'Tags':          None,\n",
    "        }\n",
    "        return out\n",
    "    else:\n",
    "        ## Weave Title\n",
    "        path = '/html/body/div[5]/div/table/tr/td[2]/div[2]/font'\n",
    "        title = dom.xpath(path)[0].text\n",
    "        out['Weave Title'] = title\n",
    "\n",
    "        ## Get AR Values\n",
    "\n",
    "        ### Get AR val string from page\n",
    "        path = '/html/body/div[5]/div/table/tr/td[1]/table/tr[2]/td/div[not(@class)][2]//text()'\n",
    "        ar_val_string = ' '.join([i for i in dom.xpath(path) if i != '\\n'][:-1])\n",
    "\n",
    "        ### Extract Aspect Ratio Values from the AR val string\n",
    "        max_ar = re.search('%s(.*)%s' % ('Max. AR :', ' Ideal AR :'), ar_val_string).group(1).replace(' ', '')\n",
    "        ideal_ar = re.search('%s(.*)%s' % (' Ideal AR :', 'Min. AR :'), ar_val_string).group(1).replace(' ', '')\n",
    "        min_ar = re.search('%s(.*)' % ('Min. AR :'), ar_val_string).group(1).replace(' ', '')\n",
    "\n",
    "        ### Add AR values to output dictionary\n",
    "        out.update({'Max AR': max_ar, 'Ideal AR': ideal_ar, 'Min AR': min_ar})\n",
    "\n",
    "\n",
    "        ## Get Dates\n",
    "\n",
    "        ### Get date string\n",
    "        path = '/html/body/div[5]/div/table/tr/td[1]/table/tr[2]/td/div[not(@class)][3]//text()'\n",
    "        results = dom.xpath(path)\n",
    "        date_string = ''.join(results).replace('\\n', '')\n",
    "\n",
    "        ### Extract Dates from date string\n",
    "        date_uploaded = re.search('%s(.*)%s' % ('Date Uploaded', 'Last Edited'), date_string).group(1).strip()\n",
    "        last_edited = re.search('%s(.*)' % ('Last Edited'), date_string).group(1).strip()\n",
    "\n",
    "        ### Add AR values to output dictionary\n",
    "        out.update({'Date Uploaded': date_uploaded, 'Last Edited': last_edited})\n",
    "\n",
    "\n",
    "        ## Get Article and Tag information\n",
    "        out.update(get_articles_and_tags(dom))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Through Articles And Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define output dict\n",
    "out = dict()\n",
    "\n",
    "# Iterate through all test articles\n",
    "for article_path in glob.glob('../articles/*'):\n",
    "    article_number = int(article_path[:-5].split('_')[1])\n",
    "    with open(article_path, mode='rb') as article:\n",
    "        out[article_number] = article_parser(article.read())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save The Data For Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../data/chainmail_data.json', mode='w') as json_file:\n",
    "    json.dump(out, json_file, sort_keys=True, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd1eb147e263fe86cfc4a17abdb5e1e40ad8fbdb4017fb6b66ff85f62d5dea51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
